{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f291981c",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b539b",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, you will code a naive actor-critic algorithm in the tabular case. Then you will tune it using grid search and Bayesian optimization, potentially using the [optuna](https://optuna.readthedocs.io/en/stable/) library.\n",
    "Finally, you will get the best hyper-parameters obtained with both methods and perform a statistical test to see if there is a statistically significant difference between these methods and with respect to naive hyper-parameter values.\n",
    "\n",
    "在本笔记本中，您将实现一个简单的演员-评论家算法（naive actor-critic algorithm）在表格形式下的代码。接着，您将使用网格搜索（grid search）和贝叶斯优化（Bayesian optimization）来调整该算法的参数，可能会使用 optuna 库。最后，您将获得两种方法得到的最佳超参数，并进行统计测试，以查看这两种方法之间是否存在统计显著差异，以及与简单超参数值相比的情况。\n",
    "\n",
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec87094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "easyinstall(\"optuna\")\n",
    "easyinstall(\"gymnasium\")\n",
    "easyinstall(\"mazemdp\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List  # Ensure compatibility with Python 3.8 and below, with the same functionality as 'list'.\n",
    "\n",
    "import hydra\n",
    "import optuna\n",
    "import yaml\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# For visualization\n",
    "os.environ[\"VIDEO_FPS\"] = \"5\"\n",
    "if not os.path.isdir(\"./videos\"):\n",
    "    os.mkdir(\"./videos\")\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2e8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6281c86",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from bbrl.utils.chrono import Chrono\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mazemdp.toolbox import sample_categorical\n",
    "from mazemdp.mdp import Mdp\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from functools import partial\n",
    "\n",
    "# matplotlib.use(\"TkAgg\")\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f24c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: white;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89706b8f5d7849a882dc6b74e9d327f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0, -1,  6, 11, 15],\n",
       "       [ 1,  4,  7, 12, 16],\n",
       "       [ 2, -1,  8, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"MazeMDP-v0\",\n",
    "    kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2, \"hit\": 0.0},\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "env.reset()\n",
    "env.unwrapped.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073b453",
   "metadata": {},
   "source": [
    "# Step 1: Coding the naive Actor-critic algorithm \n",
    "# 编码简单的演员-评论家算法\n",
    "\n",
    "We consider the naive actor-critic algorithm with a categorical policy.\n",
    "The algorithm learns a critic with the standard temporal difference mechanism\n",
    "using a learning rate $\\alpha_{critic}$.\n",
    "\n",
    "We consider a value-based critic $V(s)$. The extension to an action value function $Q(s,a)$ is straightforward.\n",
    "\n",
    "To update the critic, the algorithm computes the temporal difference error:\n",
    "\n",
    "$$\\delta_t = r(s_t, a_t) + \\gamma V^{(n)}(s_{t+1})-V^{(n)}(s_t).$$\n",
    "\n",
    "Then it applies it to the critic:\n",
    "\n",
    "$$V^{(n+1)}(s_t) = V^{(n)}(s_t) + \\alpha_{critic} \\delta_t.$$\n",
    "\n",
    "To update the actor, the general idea is the same, using the temporal difference error with another learning rate $\\alpha_{actor}$.\n",
    "\n",
    "However, naively applying the same learning rule would not ensure that the probabilities of all actions in a state sum to 1.\n",
    "Besides, when the temporal difference error $\\delta_t$ is negative, it may happen that the probability of an action gets negative or null, which raises an issue when applying renormalization.\n",
    "\n",
    "So, instead of applying the naive rule, we apply the following one:\n",
    "$$ \n",
    "\\pi_{temp}(a_t|s_t) =  \\begin{cases}\n",
    "\\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t & \\mathrm{if } \\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t > 10^{-8}\\\\\n",
    "10^{-8} & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then we can apply renormalization so that the probabilities of actions still sum to 1, with\n",
    "$$\n",
    "\\forall a, \\pi^{(i+1)}(a|s_t) = \\frac{\\pi_{temp}^{(i+1)}(a|s_t)} {\\sum_{a'} \\pi_{temp}^{(i+1)}(a'|s_t)}\n",
    "$$ with\n",
    "$$ \n",
    "\\pi_{temp}^{(i+1)}(a|s_t) =  \\begin{cases}\n",
    "\\pi_{temp}(a|s_t) & \\mathrm{if } a = a_t\\\\\n",
    "\\pi^{(i)}(a|s_t) & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "### 1. Code the naive actor-critic algorithm as specified above.\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- a good idea to build this code it to take inspiration from the code of Q-learning, to add an actor (a categorical policy), both learning rates,\n",
    "and to take care about the renormalization function.\n",
    "\n",
    "- for the next steps of this lab, having a function to repeatedly call your actor-critic algorithm and save the learning trajectories and\n",
    "norms of the value function is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7643de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "# assert False, 'Not implemented yet'\n",
    "def naive_actor_critic(\n",
    "        mdp: MazeMDPEnv,\n",
    "        alpha_critic: float,\n",
    "        alpha_actor: float,\n",
    "        nb_episodes: int = 50,\n",
    "        timeout: int = 50,\n",
    "        render: bool = True\n",
    ")->tuple[np.ndarray, List[float], List[int]]:\n",
    "    # initialize the state value function and policy pi\n",
    "    v = np.zeros(mdp.nb_states)\n",
    "    pi = np.ones((mdp.nb_states,mdp.action_space.n))/mdp.action_space.n\n",
    "\n",
    "    v_norms = []\n",
    "    time_list = []\n",
    "\n",
    "    mdp.timeout = timeout  # set maximum step of each epoch\n",
    "\n",
    "    if render:\n",
    "        mdp.init_draw(\"Naive Actor-Critic\")\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        x, _ = mdp.reset(uniform=True)\n",
    "        cpt = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not(terminated or truncated):\n",
    "            if render:\n",
    "                mdp.draw_v_pi(v, pi.argmax(axis = 1))\n",
    "\n",
    "            # choose action from policy\n",
    "            u = np.random.choice(len(pi[x]), p = pi[x])\n",
    "\n",
    "            y, r, terminated, truncated, _ = mdp.step(u)\n",
    "\n",
    "            # update critic(value function)\n",
    "            delta = r + mdp.gamma*(1-terminated) * v[y] - v[x]\n",
    "            v[x] = v[x] + alpha_critic * delta\n",
    "\n",
    "            # update actor (policy)\n",
    "            pi_temp = np.copy(pi[x])\n",
    "            pi_temp[u] = max(pi_temp[u]+alpha_actor*delta, 1e-8)\n",
    "\n",
    "            # normalization\n",
    "            pi[x] = pi_temp/np.sum(pi_temp)\n",
    "\n",
    "            x = y\n",
    "            cpt += 1\n",
    "            # print(f\"Episode {_}, Steps: {cpt}, Value: {v[x]}, Delta: {delta}\")\n",
    "\n",
    "        v_norms.append(np.linalg.norm(v))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(v, pi.argmax(axis = 1))\n",
    "    \n",
    "    return v, v_norms, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185c0bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f7b44f4b934ab8a3fcfa476fbfb1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0, -1,  6, 11, 15],\n",
       "       [ 1,  4,  7, 12, 16],\n",
       "       [ 2, -1,  8, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_critic = 0.5\n",
    "alpha_actor = 0.5\n",
    "\n",
    "v, v_norms, time_list = naive_actor_critic(env.unwrapped,alpha_critic=alpha_critic, alpha_actor=alpha_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da53e73",
   "metadata": {},
   "source": [
    "### 2. Provide a plot function\n",
    "\n",
    "Your plot function should show the evolution through time of number of steps the agent takes to find the reward in the maze.\n",
    "If your algorithm works, this number of steps should decrease through time.\n",
    "\n",
    "Your plot function should also show a mean and a standard deviation (or some more advanced statistics) over a collection of learning runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f00ef3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "# assert False, 'Not implemented yet'\n",
    "\n",
    "def plot_steps_evaluation(\n",
    "        mdp: MazeMDPEnv,\n",
    "        alpha_critic: float,\n",
    "        alpha_actor: float,\n",
    "        nb_episodes: int = 30,\n",
    "        timeout: int = 50,\n",
    "        nb_repeats: int = 10,\n",
    "        render: bool = False\n",
    "):\n",
    "    time_lists = []\n",
    "    for run in range(nb_repeats):\n",
    "        _, _, time_list = naive_actor_critic(mdp, alpha_critic, alpha_actor, nb_episodes, timeout, render)\n",
    "        time_lists.append(time_list)\n",
    "    mean_steps = np.mean(time_lists, axis = 0)\n",
    "    std_steps = np.std(time_lists,axis = 0)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(mean_steps, label = \"Mean Steps\", color = 'b')\n",
    "    plt.fill_between(range(nb_episodes), mean_steps-std_steps, mean_steps+std_steps, color = 'b', alpha = 0.2, label = \"Standard Deviation\")\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Steps to find reward')\n",
    "    plt.title('Steps to Find Reward Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(\"output.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4acf6",
   "metadata": {},
   "source": [
    "## Actor-critic hyper-parameters 超参数\n",
    "\n",
    "To represent the hyper-parameters of the experiments performed in this notebook, we suggest using the dictionary below.\n",
    "This dictionary can be read using omegaconf.\n",
    "Using it is not mandatory.\n",
    "You can also change the value of hyper-parameters or environment parameters at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a685f967",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ac_params = {\n",
    "    \"save_curves\": False,\n",
    "    \"save_heatmap\": True,\n",
    "    \"mdp\": {\n",
    "        \"name\": \"MazeMDP-v0\",\n",
    "        \"width\": 5,\n",
    "        \"height\": 5,\n",
    "        \"ratio\": 0.2,\n",
    "        \"render_mode\": \"human\", # \"rgb_array\"\n",
    "        },\n",
    "        \n",
    "    \"log_dir\": \"./tmp\",\n",
    "    \"video_dir\": \"./tmp/videos\",\n",
    "\n",
    "    \"nb_episodes\": 100,\n",
    "    \"timeout\": 200,\n",
    "    \"render\": False, # True, # \n",
    "    \"nb_repeats\": 5,\n",
    "\n",
    "    \"alpha_critic\": 0.5,\n",
    "    \"alpha_actor\": 0.5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370af64a",
   "metadata": {},
   "source": [
    "### 3. Test your code\n",
    "\n",
    "Once everything looks OK, save the obtained plot for your lab report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316bb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "config = OmegaConf.create(ac_params)\n",
    "env = gym.make(\n",
    "        config.mdp.name, \n",
    "        kwargs={\"width\":config.mdp.width, \"height\": config.mdp.height, \"ratio\": config.mdp.ratio},\n",
    "        render_mode = config.mdp.render_mode,\n",
    ")\n",
    "\n",
    "\n",
    "plot_steps_evaluation(\n",
    "  env.unwrapped,\n",
    "  alpha_critic = config.alpha_critic,\n",
    "  alpha_actor = config.alpha_actor,\n",
    "  nb_episodes=config.nb_episodes,\n",
    "  timeout=config.timeout,\n",
    "  nb_repeats=config.nb_repeats,\n",
    "  render=config.render\n",
    "    )\n",
    "\n",
    "# assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ad6ba",
   "metadata": {},
   "source": [
    "# Step 2: Tuning hyper-parameters\n",
    "\n",
    "In this part, you have to optimize two hyper-parameters of the actor-critic algorithm, namely the actor and critic learning rates.\n",
    "You have to do so using a simple grid search method and some Bayesian optimization method.\n",
    "For the latter, we suggest using the default sampler from [optuna](https://optuna.readthedocs.io/en/stable/).\n",
    "Follow the above link to understand how optuna works.\n",
    "Note that it also supports grid search and many other hyper-parameters tuning algorithms.\n",
    "\n",
    "You should make sure that the hyper-parameters tuning algorithms that you compare benefit from the same training budget\n",
    "We suggest 400 training runs overall for each method,\n",
    "which means 20 values each for the actor and the critic learning rates in the case of grid search.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "### 1. Perform hyper-parameters tuning with two algorithms as suggested above.\n",
    "\n",
    "### 2. Provide a \"heatmap\" of the norm of the value function given the hyper-parameters, after training for each pair of hyper-parameters.\n",
    "\n",
    "### 3. Collect the value of the best hyper-parameters found with each algorithm. You will need them for Step 3.\n",
    "\n",
    "### 4. Include in your report the heatmaps and the best hyper-parameters found for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "251a2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|██████████| 400/400 [00:30<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9526315789473684, 0.8105263157894737)\n",
      "2.8174140391424167\n"
     ]
    }
   ],
   "source": [
    "# grid search methode\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def grid_search(\n",
    "            mdp:MazeMDPEnv,\n",
    "            alpha_critic_values: np.ndarray,\n",
    "            alpha_actor_values: np.ndarray,\n",
    "            nb_episodes: int = 50,\n",
    "            timeout: int = 50,\n",
    "            nb_repeats: int = 1,\n",
    "            render: bool = False,\n",
    "            plot_heatmap: bool = True\n",
    ") -> tuple[tuple[float, float], float, list[tuple[float, float, float]]]:\n",
    "    \n",
    "    best_v_norm = float('-inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    param_grid = list(itertools.product(alpha_critic_values,alpha_actor_values))\n",
    "\n",
    "    for alpha_critic,alpha_actor in tqdm(param_grid, desc = \"Grid Search\"):\n",
    "        v_norms_repeat = []\n",
    "        for _ in range(nb_repeats):\n",
    "            v, v_norms, time_list = naive_actor_critic(mdp, alpha_critic, alpha_actor, nb_episodes, timeout, render)\n",
    "            v_norms_repeat.append(v_norms[-1])\n",
    "        \n",
    "        avg_v_norm = np.mean(v_norms_repeat)\n",
    "        results.append((round(alpha_critic,2), round(alpha_actor,2), avg_v_norm))\n",
    "        if avg_v_norm > best_v_norm:\n",
    "            best_v_norm = avg_v_norm\n",
    "            best_params = (alpha_critic, alpha_actor)\n",
    "    \n",
    "    if plot_heatmap:\n",
    "        plt.clf() \n",
    "        df = pd.DataFrame(results, columns=['alpha_critic', 'alpha_actor', 'v_norm'])\n",
    "        sns.heatmap(df.pivot(index='alpha_critic', columns='alpha_actor', values='v_norm'), annot=False, cmap='viridis')\n",
    "        plt.title('Heatmap of Value Function Norm With Grid Search')\n",
    "        plt.xlabel('Alpha Critic')\n",
    "        plt.ylabel('Alpha Actor')\n",
    "        plt.show()\n",
    "        plt.savefig('heatmap_grid.png')\n",
    "\n",
    "    return best_params, best_v_norm, results\n",
    "alpha_critic_values = np.linspace(0.1, 1, 20)\n",
    "alpha_actor_values = np.linspace(0.1, 1, 20)\n",
    "best_params_grid, best_v_nrom, results_grid = grid_search(env.unwrapped,alpha_critic_values, alpha_actor_values, nb_repeats=1)\n",
    "print(best_params_grid)\n",
    "print(best_v_nrom)\n",
    "# print(results_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79c5bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-01 20:29:17,306] A new study created in memory with name: no-name-75659367-7dc0-4980-ba3f-874ee9bf6d05\n",
      "[I 2024-10-01 20:29:17,418] Trial 0 finished with value: 2.5195883402028114 and parameters: {'alpha_critic': 0.9309463095870434, 'alpha_actor': 0.1884931325257185}. Best is trial 0 with value: 2.5195883402028114.\n",
      "[I 2024-10-01 20:29:17,547] Trial 1 finished with value: 1.4384292599492439 and parameters: {'alpha_critic': 0.11342742401054771, 'alpha_actor': 0.8915151951479235}. Best is trial 0 with value: 2.5195883402028114.\n",
      "[I 2024-10-01 20:29:17,635] Trial 2 finished with value: 2.115393848372981 and parameters: {'alpha_critic': 0.43389481330318114, 'alpha_actor': 0.6337427509173861}. Best is trial 0 with value: 2.5195883402028114.\n",
      "[I 2024-10-01 20:29:17,721] Trial 3 finished with value: 2.0421874685550327 and parameters: {'alpha_critic': 0.33665672744794684, 'alpha_actor': 0.8924734189126052}. Best is trial 0 with value: 2.5195883402028114.\n",
      "[I 2024-10-01 20:29:17,797] Trial 4 finished with value: 2.0291122441456575 and parameters: {'alpha_critic': 0.3486991198188659, 'alpha_actor': 0.5239084589991226}. Best is trial 0 with value: 2.5195883402028114.\n",
      "[I 2024-10-01 20:29:17,867] Trial 5 finished with value: 2.5913618390050486 and parameters: {'alpha_critic': 0.7744228823855277, 'alpha_actor': 0.41788166412763117}. Best is trial 5 with value: 2.5913618390050486.\n",
      "[I 2024-10-01 20:29:17,945] Trial 6 finished with value: 2.245279572262997 and parameters: {'alpha_critic': 0.38393635154391337, 'alpha_actor': 0.7407906085171059}. Best is trial 5 with value: 2.5913618390050486.\n",
      "[I 2024-10-01 20:29:18,019] Trial 7 finished with value: 2.5587682252334 and parameters: {'alpha_critic': 0.7876120950595215, 'alpha_actor': 0.345726345276929}. Best is trial 5 with value: 2.5913618390050486.\n",
      "[I 2024-10-01 20:29:18,079] Trial 8 finished with value: 2.6337396911368827 and parameters: {'alpha_critic': 0.7894628911516882, 'alpha_actor': 0.8900914331792377}. Best is trial 8 with value: 2.6337396911368827.\n",
      "[I 2024-10-01 20:29:18,172] Trial 9 finished with value: 2.3528283157325616 and parameters: {'alpha_critic': 0.6146981698334808, 'alpha_actor': 0.4717929101311903}. Best is trial 8 with value: 2.6337396911368827.\n",
      "[I 2024-10-01 20:29:18,235] Trial 10 finished with value: 2.722994805901077 and parameters: {'alpha_critic': 0.9956241993662942, 'alpha_actor': 0.9943756267585558}. Best is trial 10 with value: 2.722994805901077.\n",
      "[I 2024-10-01 20:29:18,348] Trial 11 finished with value: 2.192650554027876 and parameters: {'alpha_critic': 0.9723226313408543, 'alpha_actor': 0.9933920432411344}. Best is trial 10 with value: 2.722994805901077.\n",
      "[I 2024-10-01 20:29:18,436] Trial 12 finished with value: 2.673328333026982 and parameters: {'alpha_critic': 0.8081404991223275, 'alpha_actor': 0.7415739534738852}. Best is trial 10 with value: 2.722994805901077.\n",
      "[I 2024-10-01 20:29:18,500] Trial 13 finished with value: 2.7968056339819247 and parameters: {'alpha_critic': 0.994620591233014, 'alpha_actor': 0.7066681300366767}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:18,569] Trial 14 finished with value: 2.723687836586824 and parameters: {'alpha_critic': 0.9855277665964592, 'alpha_actor': 0.6908368608951154}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:18,644] Trial 15 finished with value: 2.6685616462943638 and parameters: {'alpha_critic': 0.6760472048133171, 'alpha_actor': 0.6588094728784235}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:18,725] Trial 16 finished with value: 2.7499559238823847 and parameters: {'alpha_critic': 0.882524893243425, 'alpha_actor': 0.6136697049804515}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:18,830] Trial 17 finished with value: 2.429743200462188 and parameters: {'alpha_critic': 0.8782558773736473, 'alpha_actor': 0.5792986192850642}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:18,940] Trial 18 finished with value: 2.2163221022245962 and parameters: {'alpha_critic': 0.5032755156139054, 'alpha_actor': 0.30213679785763}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,011] Trial 19 finished with value: 2.446394323261705 and parameters: {'alpha_critic': 0.6575003862491624, 'alpha_actor': 0.8110790930005314}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,119] Trial 20 finished with value: 2.2829481994972496 and parameters: {'alpha_critic': 0.8596759756362996, 'alpha_actor': 0.11775706241988881}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,177] Trial 21 finished with value: 2.623995496684504 and parameters: {'alpha_critic': 0.9023492933468329, 'alpha_actor': 0.6640843452416234}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,250] Trial 22 finished with value: 2.7879352399814454 and parameters: {'alpha_critic': 0.9675401821795299, 'alpha_actor': 0.7572286840219251}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,309] Trial 23 finished with value: 2.705917480597988 and parameters: {'alpha_critic': 0.8658379414968872, 'alpha_actor': 0.7924539442984629}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,396] Trial 24 finished with value: 2.532414918841093 and parameters: {'alpha_critic': 0.7154887395299709, 'alpha_actor': 0.5748882180348018}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,469] Trial 25 finished with value: 2.6895542025492736 and parameters: {'alpha_critic': 0.9421065860812042, 'alpha_actor': 0.8078885114882067}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,582] Trial 26 finished with value: 1.743207498627244 and parameters: {'alpha_critic': 0.21953342958736483, 'alpha_actor': 0.5197020983442913}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,642] Trial 27 finished with value: 2.4787350989276384 and parameters: {'alpha_critic': 0.5399987785625301, 'alpha_actor': 0.611914335133233}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,716] Trial 28 finished with value: 2.592164202109571 and parameters: {'alpha_critic': 0.8404411506431921, 'alpha_actor': 0.7216465665703544}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,834] Trial 29 finished with value: 2.6100168717490058 and parameters: {'alpha_critic': 0.9329448820407794, 'alpha_actor': 0.4419925289842136}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:19,988] Trial 30 finished with value: 2.6344966560441456 and parameters: {'alpha_critic': 0.7180181353311492, 'alpha_actor': 0.8399122690837658}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,254] Trial 31 finished with value: 2.7434806847108573 and parameters: {'alpha_critic': 0.9991489866023494, 'alpha_actor': 0.6879684150696962}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,338] Trial 32 finished with value: 2.7816440684467736 and parameters: {'alpha_critic': 0.9182726442790121, 'alpha_actor': 0.7600867106412857}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,420] Trial 33 finished with value: 2.617012155621573 and parameters: {'alpha_critic': 0.9290997057881815, 'alpha_actor': 0.7611073766840392}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,561] Trial 34 finished with value: 2.527369197918913 and parameters: {'alpha_critic': 0.9141101176252738, 'alpha_actor': 0.8718896305077768}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,638] Trial 35 finished with value: 2.683120909049925 and parameters: {'alpha_critic': 0.8304857032289288, 'alpha_actor': 0.9235367530340713}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,723] Trial 36 finished with value: 2.659352456858453 and parameters: {'alpha_critic': 0.7600805086205284, 'alpha_actor': 0.604646179572234}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,845] Trial 37 finished with value: 1.4958905488923038 and parameters: {'alpha_critic': 0.13824590459654068, 'alpha_actor': 0.639655937487644}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,908] Trial 38 finished with value: 2.704158629528653 and parameters: {'alpha_critic': 0.9416625870738232, 'alpha_actor': 0.506154516326072}. Best is trial 13 with value: 2.7968056339819247.\n",
      "[I 2024-10-01 20:29:20,963] Trial 39 finished with value: 2.8107816104845753 and parameters: {'alpha_critic': 0.8822643420164508, 'alpha_actor': 0.9342835456875935}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,021] Trial 40 finished with value: 2.5880811292984087 and parameters: {'alpha_critic': 0.7535808184714834, 'alpha_actor': 0.9405004930648254}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,079] Trial 41 finished with value: 2.651611407324876 and parameters: {'alpha_critic': 0.8851146696410404, 'alpha_actor': 0.7664737628555148}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,157] Trial 42 finished with value: 2.7642676759540628 and parameters: {'alpha_critic': 0.9571101955951553, 'alpha_actor': 0.707393113461557}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,221] Trial 43 finished with value: 2.6139532887072936 and parameters: {'alpha_critic': 0.9615075150475245, 'alpha_actor': 0.8432515717711838}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,345] Trial 44 finished with value: 2.7387240363965613 and parameters: {'alpha_critic': 0.9997658003324398, 'alpha_actor': 0.960585598001837}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,429] Trial 45 finished with value: 2.6904666744439587 and parameters: {'alpha_critic': 0.8251795071457985, 'alpha_actor': 0.7221921619271772}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,527] Trial 46 finished with value: 2.6765273111702794 and parameters: {'alpha_critic': 0.9544415604435603, 'alpha_actor': 0.8649906806484966}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,598] Trial 47 finished with value: 2.6293882795219408 and parameters: {'alpha_critic': 0.9070054254199427, 'alpha_actor': 0.9116524277418407}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,668] Trial 48 finished with value: 2.2907749993350826 and parameters: {'alpha_critic': 0.44151503998543457, 'alpha_actor': 0.6974475957679979}. Best is trial 39 with value: 2.8107816104845753.\n",
      "[I 2024-10-01 20:29:21,749] Trial 49 finished with value: 2.570355802689537 and parameters: {'alpha_critic': 0.7862543745527966, 'alpha_actor': 0.7763170552428379}. Best is trial 39 with value: 2.8107816104845753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8822643420164508, 0.9342835456875935)\n"
     ]
    }
   ],
   "source": [
    "# Bayesian optimization method\n",
    "\n",
    "def bayesian_optimization(\n",
    "        mdp: MazeMDPEnv,\n",
    "        nb_episodes: int = 50,\n",
    "        timeout: int = 50,\n",
    "        nb_repeats: int = 1,\n",
    "        n_trials: int = 50,\n",
    "        render: bool = False,\n",
    "        plot_heatmap: bool = True\n",
    ") -> tuple[tuple[float, float], list[tuple[float, float, float]]]:\n",
    "    def objective(trial):\n",
    "        alpha_critic = trial.suggest_float('alpha_critic', 0.1, 1)\n",
    "        alpha_actor = trial.suggest_float('alpha_actor', 0.1, 1)\n",
    "\n",
    "        v_norms_repeat = []\n",
    "        for _ in range(nb_repeats):\n",
    "            v, v_norms, time_list = naive_actor_critic(mdp, alpha_critic, alpha_actor, nb_episodes, timeout, render)\n",
    "            v_norms_repeat.append(v_norms[-1])\n",
    "\n",
    "        avg_v_norm = np.mean(v_norms_repeat)\n",
    "        return avg_v_norm\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = (study.best_params['alpha_critic'], study.best_params['alpha_actor'])\n",
    "    # results = [(trial.params['alpha_critic'], trial.params['alpha_actor'], trial.value) for trial in study.trials]\n",
    "    results = [(round(trial.params['alpha_critic'], 2), round(trial.params['alpha_actor'], 2), trial.value) for trial in study.trials]\n",
    "\n",
    "    if plot_heatmap:\n",
    "        plt.clf()\n",
    "        df = pd.DataFrame(results, columns=['alpha_critic', 'alpha_actor', 'v_norm'])\n",
    "        sns.heatmap(df.pivot(index='alpha_critic', columns='alpha_actor', values='v_norm'), annot=True, fmt=\".2f\", cmap='viridis')\n",
    "        plt.title('Heatmap of Value Function Norm With Bayesian Optimization')\n",
    "        plt.xlabel('Alpha Critic')\n",
    "        plt.ylabel('Alpha Actor')\n",
    "        plt.show()\n",
    "        plt.savefig('heatmap_bayesian.png')\n",
    "    return best_params, results\n",
    "\n",
    "best_params_bayesian, results_bayesian = bayesian_optimization(env.unwrapped,nb_repeats=1)\n",
    "print(best_params_bayesian)\n",
    "# print(results_bayesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8cd8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8822643420164508, 0.9342835456875935)\n",
      "2.8107816104845753\n",
      "[(0.93, 0.19, 2.5195883402028114), (0.11, 0.89, 1.4384292599492439), (0.43, 0.63, 2.115393848372981), (0.34, 0.89, 2.0421874685550327), (0.35, 0.52, 2.0291122441456575), (0.77, 0.42, 2.5913618390050486), (0.38, 0.74, 2.245279572262997), (0.79, 0.35, 2.5587682252334), (0.79, 0.89, 2.6337396911368827), (0.61, 0.47, 2.3528283157325616), (1.0, 0.99, 2.722994805901077), (0.97, 0.99, 2.192650554027876), (0.81, 0.74, 2.673328333026982), (0.99, 0.71, 2.7968056339819247), (0.99, 0.69, 2.723687836586824), (0.68, 0.66, 2.6685616462943638), (0.88, 0.61, 2.7499559238823847), (0.88, 0.58, 2.429743200462188), (0.5, 0.3, 2.2163221022245962), (0.66, 0.81, 2.446394323261705), (0.86, 0.12, 2.2829481994972496), (0.9, 0.66, 2.623995496684504), (0.97, 0.76, 2.7879352399814454), (0.87, 0.79, 2.705917480597988), (0.72, 0.57, 2.532414918841093), (0.94, 0.81, 2.6895542025492736), (0.22, 0.52, 1.743207498627244), (0.54, 0.61, 2.4787350989276384), (0.84, 0.72, 2.592164202109571), (0.93, 0.44, 2.6100168717490058), (0.72, 0.84, 2.6344966560441456), (1.0, 0.69, 2.7434806847108573), (0.92, 0.76, 2.7816440684467736), (0.93, 0.76, 2.617012155621573), (0.91, 0.87, 2.527369197918913), (0.83, 0.92, 2.683120909049925), (0.76, 0.6, 2.659352456858453), (0.14, 0.64, 1.4958905488923038), (0.94, 0.51, 2.704158629528653), (0.88, 0.93, 2.8107816104845753), (0.75, 0.94, 2.5880811292984087), (0.89, 0.77, 2.651611407324876), (0.96, 0.71, 2.7642676759540628), (0.96, 0.84, 2.6139532887072936), (1.0, 0.96, 2.7387240363965613), (0.83, 0.72, 2.6904666744439587), (0.95, 0.86, 2.6765273111702794), (0.91, 0.91, 2.6293882795219408), (0.44, 0.7, 2.2907749993350826), (0.79, 0.78, 2.570355802689537)]\n"
     ]
    }
   ],
   "source": [
    "print(best_params_bayesian)\n",
    "print(np.max(results_bayesian))\n",
    "print(results_bayesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba521f",
   "metadata": {},
   "source": [
    "# Step 3: Statistical tests\n",
    "\n",
    "Now you have to compare the performance of the actor-critic algorithm tuned\n",
    "with all the best hyper-parameters you found before, using statistical tests.\n",
    "\n",
    "The functions below are provided to run Welch's T-test over learning curves.\n",
    "They have been adapted from a github repository: https://github.com/flowersteam/rl_stats\n",
    "You don't need to understand them in detail (though it is always a good idea to try to understand more code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2258ad0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.compare_functions as bs_compare\n",
    "import bootstrapped.stats_functions as bs_stats\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba2201b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_central_tendency_and_error(id_central, id_error, sample):\n",
    "\n",
    "    try:\n",
    "        id_error = int(id_error)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if id_central == \"mean\":\n",
    "        central = np.nanmean(sample, axis=1) \n",
    "    elif id_central == \"median\":\n",
    "        central = np.nanmedian(sample, axis=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if isinstance(id_error, int):\n",
    "        low = np.nanpercentile(sample, q=int((100 - id_error) / 2), axis=1)\n",
    "        high = np.nanpercentile(sample, q=int(100 - (100 - id_error) / 2), axis=1)\n",
    "    elif id_error == \"std\":\n",
    "        low = central - np.nanstd(sample, axis=1)\n",
    "        high = central + np.nanstd(sample, axis=1)\n",
    "    elif id_error == \"sem\":\n",
    "        low = central - np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "        high = central + np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return central, low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f30e0af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_test(test_id, data1, data2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute tests comparing data1 and data2 with confidence level alpha\n",
    "    :param test_id: (str) refers to what test should be used\n",
    "    :param data1: (np.ndarray) sample 1\n",
    "    :param data2: (np.ndarray) sample 2\n",
    "    :param alpha: (float) confidence level of the test\n",
    "    :return: (bool) if True, the null hypothesis is rejected\n",
    "    \"\"\"\n",
    "    data1 = data1.squeeze()\n",
    "    data2 = data2.squeeze()\n",
    "    n1 = data1.size\n",
    "    n2 = data2.size\n",
    "\n",
    "    # perform Welch t-test\":\n",
    "    _, p = ttest_ind(data1, data2, equal_var=False)\n",
    "    return p < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058af0c",
   "metadata": {},
   "source": [
    "This last function was adapted for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24227c11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_test(perf1, perf2, name1, name2, sample_size=20, downsampling_fact=5, confidence_level=0.01):\n",
    "\n",
    "    perf1 = perf1.transpose()\n",
    "    perf2 = perf2.transpose()\n",
    "    nb_datapoints = perf1.shape[1]\n",
    "    nb_steps = perf1.shape[0]\n",
    "\n",
    "    legend = [name1, name2]\n",
    "\n",
    "    # what do you want to plot ?\n",
    "    id_central = 'mean' # \"median\"  # \n",
    "    id_error = 80  # (percentiles), also: 'std', 'sem'\n",
    "\n",
    "    test_id = \"Welch t-test\"  # recommended\n",
    "    \n",
    "    # 从发0到nb_datapoints中随机抽取sample_size个样本\n",
    "    sample1 = perf1[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "    sample2 = perf2[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "\n",
    "    # 对perf的列进行降采样\n",
    "    steps = np.arange(0, nb_steps, downsampling_fact)\n",
    "    sample1 = sample1[steps, :]\n",
    "    sample2 = sample2[steps, :]\n",
    "\n",
    "    # test\n",
    "    sign_diff = np.zeros([len(steps)])\n",
    "    for i in range(len(steps)):\n",
    "        sign_diff[i] = run_test(\n",
    "            test_id, sample1[i, :], sample2[i, :], alpha=confidence_level\n",
    "        )\n",
    "\n",
    "    central1, low1, high1 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample1\n",
    "    )\n",
    "    central2, low2, high2 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample2\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    _, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    lab1 = plt.xlabel(\"training steps\")\n",
    "    lab2 = plt.ylabel(\"performance\")\n",
    "\n",
    "    plt.plot(steps, central1, linewidth=10)\n",
    "    plt.plot(steps, central2, linewidth=10)\n",
    "    plt.fill_between(steps, low1, high1, alpha=0.3)\n",
    "    plt.fill_between(steps, low2, high2, alpha=0.3)\n",
    "    leg = ax.legend(legend, frameon=False)\n",
    "\n",
    "    # plot significative difference as dots\n",
    "    idx = np.argwhere(sign_diff == 1)\n",
    "    y = max(np.nanmax(high1), np.nanmax(high2))\n",
    "    plt.scatter(steps[idx], y * 1.05 * np.ones([idx.size]), s=100, c=\"k\", marker=\"o\")\n",
    "\n",
    "    # style\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(10.0)\n",
    "    ax.spines[\"top\"].set_linewidth(5)\n",
    "    ax.spines[\"right\"].set_linewidth(5)\n",
    "    ax.spines[\"bottom\"].set_linewidth(5)\n",
    "    ax.spines[\"left\"].set_linewidth(5)\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"./{name1}_{name2}.png\", bbox_extra_artists=(leg, lab1, lab2), bbox_inches=\"tight\", dpi=100\n",
    "    )\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1567ee0",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "As hyper-parameters, you will use:\n",
    "\n",
    "- naive tuning, that is a pair (0.5, 0.5) for the actor and critic learning rates,\n",
    "- the best hyper-parameters you found with the different tuning algorithms you used before.\n",
    "\n",
    "### 1. For each set of hyper-parameters, collect a large dataset of learning curves.\n",
    "\n",
    "We suggest using 150 training episodes.\n",
    "\n",
    "### 2. Perform statistical comparisons\n",
    "\n",
    "- Take two datasets of learning curves obtained with the hyper-parameters sets that you found with different tuning algorithms.\n",
    "- Use the ``` perform_test(...)``` function to compare each possible pair of sets.\n",
    "\n",
    "You should obtain an image for each pair you have tried.\n",
    "In this image, black dots signal the time step where there is a statistically significant difference between two learning curves.\n",
    "\n",
    " ### 3. Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ad1626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 150\n",
    "nb_repeats = 10\n",
    "\n",
    "alpha_critic, alpha_actor = (0.5, 0.5)\n",
    "alpha_critic_grid,alpha_actor_grid = best_params_grid\n",
    "alpha_critic_bayesian,alpha_actor_bayesian = best_params_bayesian\n",
    "\n",
    "Lists_1,Lists_2,Lists_3 = [],[],[]\n",
    "for _ in range(nb_repeats):\n",
    "    v_1, v_norms_1, time_list_1 = naive_actor_critic(env.unwrapped,alpha_critic=alpha_critic, alpha_actor=alpha_actor, nb_episodes=nb_episodes, render=False)\n",
    "    v_2, v_norms_2, time_list_2 = naive_actor_critic(env.unwrapped,alpha_critic=alpha_critic_grid, alpha_actor=alpha_actor_grid,nb_episodes=nb_episodes, render=False)\n",
    "    v_3, v_norms_3, time_list_3 = naive_actor_critic(env.unwrapped,alpha_critic=alpha_critic_bayesian, alpha_actor=alpha_actor_bayesian, nb_episodes=nb_episodes, render=False)\n",
    "    Lists_1.append(time_list_1)\n",
    "    Lists_2.append(time_list_2)\n",
    "    Lists_3.append(time_list_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f66f0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves = {}\n",
    "\n",
    "learning_curves[\"naive\"] = np.array(Lists_1)\n",
    "learning_curves[\"grid\"] = np.array(Lists_2)\n",
    "learning_curves[\"bayesian\"] = np.array(Lists_3)\n",
    "\n",
    "comparaison_results = {}\n",
    "for name1 in learning_curves.keys():\n",
    "    for name2 in learning_curves.keys():\n",
    "        if name1 != name2:\n",
    "            comparaison_results[(name1, name2)] = perform_test(\n",
    "                learning_curves[name1],\n",
    "                learning_curves[name2],\n",
    "                name1 = name1,\n",
    "                name2 = name2,\n",
    "                confidence_level=0.05,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa21a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fae786",
   "metadata": {},
   "source": [
    "# Lab report\n",
    "\n",
    "Your report should contain:\n",
    "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
    "- in a separate pdf file with your names in the name of the file:\n",
    "    + a detailed enough description of the choices you have made: the parameters you have set, the libraries you have used, etc.,\n",
    "    + the heatmaps obtained using the hyper-parameters tuning algorithms that you have used,\n",
    "    + the figures resulting from performing Welch's T-test using the best hyper-parameters from the above approaches,\n",
    "    + your conclusion from these experiments.\n",
    "\n",
    "Beyond the elements required in this report, any additional studies will be rewarded.\n",
    "For instance, you can try using a Q-function as critic, using random search as hyper-parameters tuning algorithm,\n",
    "using more challenging environments, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c7681",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
